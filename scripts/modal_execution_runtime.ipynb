{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modal Execution Runtime for ML Experiments\n",
    "\n",
    "This notebook provides a complete execution runtime for running ML experiments on Modal serverless infrastructure.\n",
    "\n",
    "## Features:\n",
    "- Spin up Modal serverless instances\n",
    "- Execute experiment Python scripts with JSON criteria\n",
    "- Return in-progress job IDs for polling\n",
    "- Collect and compare benchmarks across multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import asyncio\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from dataclasses import dataclass, asdict\n",
    "import tempfile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modal Configuration and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Modal (run this once)\n",
    "# You'll need to have Modal CLI installed: pip install modal\n",
    "# Then run: modal token new\n",
    "# Or set MODAL_TOKEN_ID and MODAL_TOKEN_SECRET environment variables\n",
    "\n",
    "# Create Modal app\n",
    "app = modal.App(\"ml-experiment-runner\")\n",
    "\n",
    "# Define the image with all required dependencies\n",
    "image = modal.Image.debian_slim().pip_install(\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\"\n",
    ")\n",
    "\n",
    "# Create a volume for storing experiment artifacts\n",
    "volume = modal.Volume.from_name(\"experiment-artifacts\", create_if_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for an ML experiment\"\"\"\n",
    "    experiment_id: str\n",
    "    model_config: Dict[str, Any]\n",
    "    dataset: str = \"cifar10\"\n",
    "    metrics: List[str] = None\n",
    "    baselines: List[str] = None\n",
    "    training_params: Dict[str, Any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metrics is None:\n",
    "            self.metrics = [\"accuracy\", \"f1_score\", \"loss\"]\n",
    "        if self.baselines is None:\n",
    "            self.baselines = [\"cnn_basic\", \"resnet18\"]\n",
    "        if self.training_params is None:\n",
    "            self.training_params = {\n",
    "                \"epochs\": 3,\n",
    "                \"batch_size\": 128,\n",
    "                \"learning_rate\": 0.001\n",
    "            }\n",
    "\n",
    "@dataclass\n",
    "class JobStatus:\n",
    "    \"\"\"Status of a running job\"\"\"\n",
    "    job_id: str\n",
    "    experiment_id: str\n",
    "    status: str  # 'pending', 'running', 'completed', 'failed'\n",
    "    started_at: datetime\n",
    "    completed_at: Optional[datetime] = None\n",
    "    error: Optional[str] = None\n",
    "    results: Optional[Dict[str, Any]] = None\n",
    "    artifacts_path: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Results from a benchmark run\"\"\"\n",
    "    model_name: str\n",
    "    accuracy: float\n",
    "    f1_score: float\n",
    "    loss: float\n",
    "    training_time: float\n",
    "    inference_time: float\n",
    "    loss_history: List[float]\n",
    "    accuracy_history: List[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Job Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobManager:\n",
    "    \"\"\"Manages experiment jobs and their statuses\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.jobs: Dict[str, JobStatus] = {}\n",
    "        self.results: Dict[str, List[BenchmarkResult]] = {}\n",
    "    \n",
    "    def create_job(self, experiment_id: str) -> str:\n",
    "        \"\"\"Create a new job and return its ID\"\"\"\n",
    "        job_id = f\"job_{uuid.uuid4().hex[:8]}\"\n",
    "        self.jobs[job_id] = JobStatus(\n",
    "            job_id=job_id,\n",
    "            experiment_id=experiment_id,\n",
    "            status='pending',\n",
    "            started_at=datetime.now()\n",
    "        )\n",
    "        return job_id\n",
    "    \n",
    "    def update_job_status(self, job_id: str, status: str, **kwargs):\n",
    "        \"\"\"Update job status\"\"\"\n",
    "        if job_id in self.jobs:\n",
    "            self.jobs[job_id].status = status\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(self.jobs[job_id], key, value)\n",
    "    \n",
    "    def get_job_status(self, job_id: str) -> Optional[JobStatus]:\n",
    "        \"\"\"Get current job status\"\"\"\n",
    "        return self.jobs.get(job_id)\n",
    "    \n",
    "    def add_benchmark_result(self, job_id: str, result: BenchmarkResult):\n",
    "        \"\"\"Add benchmark result for a job\"\"\"\n",
    "        if job_id not in self.results:\n",
    "            self.results[job_id] = []\n",
    "        self.results[job_id].append(result)\n",
    "    \n",
    "    def get_job_results(self, job_id: str) -> List[BenchmarkResult]:\n",
    "        \"\"\"Get all benchmark results for a job\"\"\"\n",
    "        return self.results.get(job_id, [])\n",
    "\n",
    "# Initialize global job manager\n",
    "job_manager = JobManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modal Serverless Execution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.function(\n",
    "    image=image,\n",
    "    volumes={'/artifacts': volume},\n",
    "    gpu=\"any\",  # Use GPU if available, otherwise CPU\n",
    "    timeout=1800,  # 30 minutes timeout\n",
    "    memory=8192,  # 8GB RAM\n",
    ")\n",
    "def run_experiment(\n",
    "    experiment_script: str,\n",
    "    config: Dict[str, Any],\n",
    "    job_id: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run an ML experiment on Modal\"\"\"\n",
    "    \n",
    "    import torch\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    from torch import nn, optim\n",
    "    from sklearn.metrics import f1_score, accuracy_score\n",
    "    import time\n",
    "    import json\n",
    "    import sys\n",
    "    from io import StringIO\n",
    "    \n",
    "    # Create artifacts directory for this job\n",
    "    artifacts_dir = Path(f\"/artifacts/{job_id}\")\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save config\n",
    "    with open(artifacts_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Execute the experiment script\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # Create a namespace for script execution\n",
    "        namespace = {\n",
    "            'config': config,\n",
    "            'artifacts_dir': str(artifacts_dir),\n",
    "            'results': {},\n",
    "            'torch': torch,\n",
    "            'torchvision': torchvision,\n",
    "            'transforms': transforms,\n",
    "            'nn': nn,\n",
    "            'optim': optim,\n",
    "            'f1_score': f1_score,\n",
    "            'accuracy_score': accuracy_score,\n",
    "        }\n",
    "        \n",
    "        # Execute the script\n",
    "        exec(experiment_script, namespace)\n",
    "        \n",
    "        # Get results from namespace\n",
    "        results = namespace.get('results', {})\n",
    "        \n",
    "        # Save results\n",
    "        with open(artifacts_dir / \"results.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Commit volume changes\n",
    "        volume.commit()\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'results': results,\n",
    "            'artifacts_path': str(artifacts_dir)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'artifacts_path': str(artifacts_dir)\n",
    "        }\n",
    "\n",
    "@app.function(\n",
    "    image=image,\n",
    "    volumes={'/artifacts': volume},\n",
    ")\n",
    "def fetch_artifacts(job_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch artifacts from a completed job\"\"\"\n",
    "    artifacts_dir = Path(f\"/artifacts/{job_id}\")\n",
    "    \n",
    "    if not artifacts_dir.exists():\n",
    "        return {'error': f'No artifacts found for job {job_id}'}\n",
    "    \n",
    "    artifacts = {}\n",
    "    \n",
    "    # Load results\n",
    "    results_file = artifacts_dir / \"results.json\"\n",
    "    if results_file.exists():\n",
    "        with open(results_file, \"r\") as f:\n",
    "            artifacts['results'] = json.load(f)\n",
    "    \n",
    "    # Load plots as base64\n",
    "    for plot_file in artifacts_dir.glob(\"*.png\"):\n",
    "        with open(plot_file, \"rb\") as f:\n",
    "            artifacts[plot_file.stem] = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    # Load config\n",
    "    config_file = artifacts_dir / \"config.json\"\n",
    "    if config_file.exists():\n",
    "        with open(config_file, \"r\") as f:\n",
    "            artifacts['config'] = json.load(f)\n",
    "    \n",
    "    return artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment Execution Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n",
    "    \"\"\"High-level interface for running experiments\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.job_manager = job_manager\n",
    "    \n",
    "    async def submit_experiment(\n",
    "        self,\n",
    "        experiment_script: str,\n",
    "        config: ExperimentConfig\n",
    "    ) -> str:\n",
    "        \"\"\"Submit an experiment for execution and return job ID\"\"\"\n",
    "        \n",
    "        # Create job\n",
    "        job_id = self.job_manager.create_job(config.experiment_id)\n",
    "        \n",
    "        # Update status to running\n",
    "        self.job_manager.update_job_status(job_id, 'running')\n",
    "        \n",
    "        try:\n",
    "            # Deploy and run on Modal\n",
    "            with app.run():\n",
    "                result = await run_experiment.remote.aio(\n",
    "                    experiment_script,\n",
    "                    asdict(config),\n",
    "                    job_id\n",
    "                )\n",
    "            \n",
    "            # Update job status based on result\n",
    "            if result['status'] == 'success':\n",
    "                self.job_manager.update_job_status(\n",
    "                    job_id,\n",
    "                    'completed',\n",
    "                    completed_at=datetime.now(),\n",
    "                    results=result['results'],\n",
    "                    artifacts_path=result['artifacts_path']\n",
    "                )\n",
    "                \n",
    "                # Parse and store benchmark results\n",
    "                self._parse_benchmark_results(job_id, result['results'])\n",
    "            else:\n",
    "                self.job_manager.update_job_status(\n",
    "                    job_id,\n",
    "                    'failed',\n",
    "                    completed_at=datetime.now(),\n",
    "                    error=result.get('error', 'Unknown error')\n",
    "                )\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.job_manager.update_job_status(\n",
    "                job_id,\n",
    "                'failed',\n",
    "                completed_at=datetime.now(),\n",
    "                error=str(e)\n",
    "            )\n",
    "        \n",
    "        return job_id\n",
    "    \n",
    "    def _parse_benchmark_results(self, job_id: str, results: Dict[str, Any]):\n",
    "        \"\"\"Parse results and create BenchmarkResult objects\"\"\"\n",
    "        for model_name, model_results in results.items():\n",
    "            if isinstance(model_results, dict):\n",
    "                benchmark = BenchmarkResult(\n",
    "                    model_name=model_name,\n",
    "                    accuracy=model_results.get('accuracy', 0.0),\n",
    "                    f1_score=model_results.get('f1_score', 0.0),\n",
    "                    loss=model_results.get('loss', 0.0),\n",
    "                    training_time=model_results.get('training_time', 0.0),\n",
    "                    inference_time=model_results.get('inference_time', 0.0),\n",
    "                    loss_history=model_results.get('loss_history', []),\n",
    "                    accuracy_history=model_results.get('accuracy_history', [])\n",
    "                )\n",
    "                self.job_manager.add_benchmark_result(job_id, benchmark)\n",
    "    \n",
    "    def poll_job(self, job_id: str) -> JobStatus:\n",
    "        \"\"\"Poll job status\"\"\"\n",
    "        return self.job_manager.get_job_status(job_id)\n",
    "    \n",
    "    async def get_artifacts(self, job_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fetch artifacts from a completed job\"\"\"\n",
    "        with app.run():\n",
    "            return await fetch_artifacts.remote.aio(job_id)\n",
    "    \n",
    "    def compare_benchmarks(self, job_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Compare benchmarks across models for a job\"\"\"\n",
    "        results = self.job_manager.get_job_results(job_id)\n",
    "        \n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = []\n",
    "        for r in results:\n",
    "            data.append({\n",
    "                'Model': r.model_name,\n",
    "                'Accuracy': r.accuracy,\n",
    "                'F1 Score': r.f1_score,\n",
    "                'Final Loss': r.loss,\n",
    "                'Training Time (s)': r.training_time,\n",
    "                'Inference Time (ms)': r.inference_time * 1000\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    def plot_training_curves(self, job_id: str):\n",
    "        \"\"\"Plot training curves for all models\"\"\"\n",
    "        results = self.job_manager.get_job_results(job_id)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        for r in results:\n",
    "            if r.loss_history:\n",
    "                ax1.plot(r.loss_history, label=r.model_name)\n",
    "            if r.accuracy_history:\n",
    "                ax2.plot(r.accuracy_history, label=r.model_name)\n",
    "        \n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Training Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create global runner instance\n",
    "runner = ExperimentRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Test Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample config saved to scripts/sample/execution/sample_config.json\n"
     ]
    }
   ],
   "source": [
    "# Create sample test inputs in scripts/sample/execution\n",
    "sample_dir = Path(\"scripts/sample/execution\")\n",
    "sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample experiment configuration\n",
    "sample_config = {\n",
    "    \"experiment_id\": \"exp_sample_001\",\n",
    "    \"model_config\": {\n",
    "        \"type\": \"cnn_custom\",\n",
    "        \"activation\": \"gelu\",\n",
    "        \"layers\": [\"conv\", \"batchnorm\", \"maxpool\"]\n",
    "    },\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"metrics\": [\"accuracy\", \"f1_score\", \"loss\"],\n",
    "    \"baselines\": [\"cnn_basic\", \"resnet18\"],\n",
    "    \"training_params\": {\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save sample config\n",
    "with open(sample_dir / \"sample_config.json\", \"w\") as f:\n",
    "    json.dump(sample_config, f, indent=2)\n",
    "\n",
    "print(f\"Sample config saved to {sample_dir / 'sample_config.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample experiment script saved to scripts/sample/execution/sample_experiment.py\n"
     ]
    }
   ],
   "source": [
    "# Sample experiment script\n",
    "sample_experiment_script = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        if activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, epochs, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['training_params']['learning_rate'])\n",
    "    \n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        loss_history.append(epoch_loss)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, Accuracy={accuracy:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Inference time\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            _ = model(inputs.to(device))\n",
    "            break\n",
    "    inference_time = (time.time() - start_time) / inputs.size(0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'loss': epoch_loss,\n",
    "        'training_time': training_time,\n",
    "        'inference_time': inference_time,\n",
    "        'loss_history': loss_history,\n",
    "        'accuracy_history': accuracy_history\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['training_params']['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['training_params']['batch_size'], shuffle=False)\n",
    "\n",
    "# Train custom model\n",
    "print(\"Training custom model...\")\n",
    "custom_model = SimpleCNN(activation=config['model_config'].get('activation', 'relu')).to(device)\n",
    "custom_results = train_model(\n",
    "    custom_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    config['training_params']['epochs'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Train baseline CNN\n",
    "print(\"Training baseline CNN...\")\n",
    "baseline_model = SimpleCNN(activation='relu').to(device)\n",
    "baseline_results = train_model(\n",
    "    baseline_model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    config['training_params']['epochs'],\n",
    "    device\n",
    ")\n",
    "\n",
    "# Store results\n",
    "results['custom_model'] = custom_results\n",
    "results['baseline_cnn'] = baseline_results\n",
    "\n",
    "print(\"Experiment completed!\")\n",
    "'''\n",
    "\n",
    "# Save sample experiment script\n",
    "with open(sample_dir / \"sample_experiment.py\", \"w\") as f:\n",
    "    f.write(sample_experiment_script)\n",
    "\n",
    "print(f\"Sample experiment script saved to {sample_dir / 'sample_experiment.py'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation criteria saved to scripts/sample/execution/evaluation_criteria.json\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation criteria JSON\n",
    "evaluation_criteria = {\n",
    "    \"success_criteria\": {\n",
    "        \"min_accuracy\": 0.6,\n",
    "        \"max_training_time\": 300,  # 5 minutes\n",
    "        \"required_metrics\": [\"accuracy\", \"f1_score\", \"loss\"]\n",
    "    },\n",
    "    \"comparison_metrics\": [\n",
    "        {\n",
    "            \"name\": \"accuracy_improvement\",\n",
    "            \"baseline\": \"baseline_cnn\",\n",
    "            \"target\": \"custom_model\",\n",
    "            \"threshold\": 0.02  # 2% improvement\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"training_efficiency\",\n",
    "            \"metric\": \"training_time\",\n",
    "            \"max_value\": 180  # 3 minutes\n",
    "        }\n",
    "    ],\n",
    "    \"output_artifacts\": [\n",
    "        \"loss_curves.png\",\n",
    "        \"accuracy_curves.png\",\n",
    "        \"benchmark_comparison.csv\",\n",
    "        \"results_summary.json\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save evaluation criteria\n",
    "with open(sample_dir / \"evaluation_criteria.json\", \"w\") as f:\n",
    "    json.dump(evaluation_criteria, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation criteria saved to {sample_dir / 'evaluation_criteria.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execution Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Submit an experiment\n",
    "async def run_sample_experiment():\n",
    "    \"\"\"Run a sample experiment\"\"\"\n",
    "    \n",
    "    # Load sample config\n",
    "    with open(\"scripts/sample/execution/sample_config.json\", \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Create experiment config\n",
    "    config = ExperimentConfig(**config_dict)\n",
    "    \n",
    "    # Load experiment script\n",
    "    with open(\"scripts/sample/execution/sample_experiment.py\", \"r\") as f:\n",
    "        script = f.read()\n",
    "    \n",
    "    # Submit experiment\n",
    "    print(\"Submitting experiment...\")\n",
    "    job_id = await runner.submit_experiment(script, config)\n",
    "    print(f\"Job submitted with ID: {job_id}\")\n",
    "    \n",
    "    # Poll for completion\n",
    "    while True:\n",
    "        status = runner.poll_job(job_id)\n",
    "        print(f\"Job status: {status.status}\")\n",
    "        \n",
    "        if status.status in ['completed', 'failed']:\n",
    "            break\n",
    "        \n",
    "        await asyncio.sleep(5)\n",
    "    \n",
    "    if status.status == 'completed':\n",
    "        print(\"\\nExperiment completed successfully!\")\n",
    "        \n",
    "        # Get benchmark comparison\n",
    "        print(\"\\nBenchmark Comparison:\")\n",
    "        comparison = runner.compare_benchmarks(job_id)\n",
    "        print(comparison)\n",
    "        \n",
    "        # Plot training curves\n",
    "        print(\"\\nPlotting training curves...\")\n",
    "        runner.plot_training_curves(job_id)\n",
    "        \n",
    "        # Fetch artifacts\n",
    "        print(\"\\nFetching artifacts...\")\n",
    "        artifacts = await runner.get_artifacts(job_id)\n",
    "        print(f\"Artifacts retrieved: {list(artifacts.keys())}\")\n",
    "    else:\n",
    "        print(f\"\\nExperiment failed: {status.error}\")\n",
    "    \n",
    "    return job_id\n",
    "\n",
    "# Run the sample experiment (uncomment to execute)\n",
    "# job_id = await run_sample_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to poll job status\n",
    "def check_job_status(job_id: str):\n",
    "    \"\"\"Check the status of a job\"\"\"\n",
    "    status = runner.poll_job(job_id)\n",
    "    \n",
    "    if status:\n",
    "        print(f\"Job ID: {status.job_id}\")\n",
    "        print(f\"Status: {status.status}\")\n",
    "        print(f\"Started: {status.started_at}\")\n",
    "        \n",
    "        if status.completed_at:\n",
    "            print(f\"Completed: {status.completed_at}\")\n",
    "            duration = (status.completed_at - status.started_at).total_seconds()\n",
    "            print(f\"Duration: {duration:.2f} seconds\")\n",
    "        \n",
    "        if status.error:\n",
    "            print(f\"Error: {status.error}\")\n",
    "        \n",
    "        if status.results:\n",
    "            print(f\"Results available: {list(status.results.keys())}\")\n",
    "    else:\n",
    "        print(f\"No job found with ID: {job_id}\")\n",
    "\n",
    "# Example usage (provide actual job_id)\n",
    "# check_job_status(\"job_12345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare multiple experiments\n",
    "def compare_experiments(job_ids: List[str]):\n",
    "    \"\"\"Compare results across multiple experiment jobs\"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for job_id in job_ids:\n",
    "        status = runner.poll_job(job_id)\n",
    "        if status and status.status == 'completed':\n",
    "            results = runner.get_job_results(job_id)\n",
    "            for r in results:\n",
    "                all_results.append({\n",
    "                    'Job ID': job_id,\n",
    "                    'Experiment': status.experiment_id,\n",
    "                    'Model': r.model_name,\n",
    "                    'Accuracy': r.accuracy,\n",
    "                    'F1 Score': r.f1_score,\n",
    "                    'Training Time': r.training_time\n",
    "                })\n",
    "    \n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(df.groupby('Model')[['Accuracy', 'F1 Score', 'Training Time']].mean())\n",
    "        \n",
    "        # Best performing model\n",
    "        best_model = df.loc[df['Accuracy'].idxmax()]\n",
    "        print(f\"\\nBest Model:\")\n",
    "        print(best_model)\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        df.groupby('Model')['Accuracy'].mean().plot(kind='bar', ax=axes[0])\n",
    "        axes[0].set_title('Average Accuracy by Model')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        \n",
    "        df.groupby('Model')['F1 Score'].mean().plot(kind='bar', ax=axes[1])\n",
    "        axes[1].set_title('Average F1 Score by Model')\n",
    "        axes[1].set_ylabel('F1 Score')\n",
    "        \n",
    "        df.groupby('Model')['Training Time'].mean().plot(kind='bar', ax=axes[2])\n",
    "        axes[2].set_title('Average Training Time by Model')\n",
    "        axes[2].set_ylabel('Time (seconds)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No completed experiments found\")\n",
    "        return None\n",
    "\n",
    "# Example usage with multiple job IDs\n",
    "# compare_experiments([\"job_id1\", \"job_id2\", \"job_id3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. API Interface for Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment API initialized and ready for integration\n"
     ]
    }
   ],
   "source": [
    "class ExperimentAPI:\n",
    "    \"\"\"API interface for integration with the FastAPI server\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.runner = runner\n",
    "    \n",
    "    async def execute_experiment(\n",
    "        self,\n",
    "        plan_id: str,\n",
    "        code_files: List[Dict[str, str]],\n",
    "        spec: Dict[str, Any]\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"Execute an experiment from API request\"\"\"\n",
    "        \n",
    "        # Create experiment config from spec\n",
    "        config = ExperimentConfig(\n",
    "            experiment_id=plan_id,\n",
    "            model_config=spec.get('model', {}),\n",
    "            dataset=spec.get('dataset', 'cifar10'),\n",
    "            metrics=spec.get('metrics', ['accuracy', 'f1']),\n",
    "            baselines=spec.get('baselines', ['cnn_basic']),\n",
    "            training_params=spec.get('train', {})\n",
    "        )\n",
    "        \n",
    "        # Combine code files into single script\n",
    "        script = self._combine_code_files(code_files)\n",
    "        \n",
    "        # Submit experiment\n",
    "        job_id = await self.runner.submit_experiment(script, config)\n",
    "        \n",
    "        return {\n",
    "            'job_id': job_id,\n",
    "            'status': 'running'\n",
    "        }\n",
    "    \n",
    "    def get_job_status(self, job_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get job status for API response\"\"\"\n",
    "        status = self.runner.poll_job(job_id)\n",
    "        \n",
    "        if not status:\n",
    "            return {'error': 'Job not found'}\n",
    "        \n",
    "        response = {\n",
    "            'job_id': status.job_id,\n",
    "            'status': status.status,\n",
    "            'started_at': status.started_at.isoformat(),\n",
    "        }\n",
    "        \n",
    "        if status.completed_at:\n",
    "            response['completed_at'] = status.completed_at.isoformat()\n",
    "        \n",
    "        if status.error:\n",
    "            response['error'] = status.error\n",
    "        \n",
    "        if status.results:\n",
    "            response['metrics'] = status.results\n",
    "            response['artifacts_path'] = status.artifacts_path\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    async def get_report_data(self, job_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get data for report generation\"\"\"\n",
    "        status = self.runner.poll_job(job_id)\n",
    "        \n",
    "        if not status or status.status != 'completed':\n",
    "            return {'error': 'Job not completed'}\n",
    "        \n",
    "        # Get benchmark comparison\n",
    "        comparison = self.runner.compare_benchmarks(job_id)\n",
    "        \n",
    "        # Get artifacts\n",
    "        artifacts = await self.runner.get_artifacts(job_id)\n",
    "        \n",
    "        return {\n",
    "            'job_id': job_id,\n",
    "            'metrics': status.results,\n",
    "            'comparison': comparison.to_dict('records') if not comparison.empty else [],\n",
    "            'artifacts': artifacts\n",
    "        }\n",
    "    \n",
    "    def _combine_code_files(self, code_files: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Combine multiple code files into a single executable script\"\"\"\n",
    "        script_parts = []\n",
    "        \n",
    "        for file_info in code_files:\n",
    "            path = file_info.get('path', '')\n",
    "            content = file_info.get('content', '')\n",
    "            \n",
    "            if path.endswith('.py'):\n",
    "                script_parts.append(f\"# File: {path}\")\n",
    "                script_parts.append(content)\n",
    "                script_parts.append(\"\\n\")\n",
    "        \n",
    "        return \"\\n\".join(script_parts)\n",
    "\n",
    "# Create global API instance\n",
    "api = ExperimentAPI()\n",
    "\n",
    "print(\"Experiment API initialized and ready for integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete flow\n",
    "async def test_complete_flow():\n",
    "    \"\"\"Test the complete experiment execution flow\"\"\"\n",
    "    \n",
    "    print(\"Starting complete flow test...\\n\")\n",
    "    \n",
    "    # 1. Load test inputs\n",
    "    with open(\"scripts/sample/execution/sample_config.json\", \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    with open(\"scripts/sample/execution/evaluation_criteria.json\", \"r\") as f:\n",
    "        criteria = json.load(f)\n",
    "    \n",
    "    print(\"✓ Test inputs loaded\")\n",
    "    \n",
    "    # 2. Create minimal test script\n",
    "    test_script = '''\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Simulate model training\n",
    "print(\"Running test experiment...\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Generate mock results\n",
    "results['test_model'] = {\n",
    "    'accuracy': 0.85 + np.random.random() * 0.1,\n",
    "    'f1_score': 0.83 + np.random.random() * 0.1,\n",
    "    'loss': 0.3 + np.random.random() * 0.2,\n",
    "    'training_time': 10.5,\n",
    "    'inference_time': 0.001,\n",
    "    'loss_history': [0.8, 0.6, 0.4, 0.3],\n",
    "    'accuracy_history': [0.6, 0.7, 0.8, 0.85]\n",
    "}\n",
    "\n",
    "print(\"Test experiment completed!\")\n",
    "'''\n",
    "    \n",
    "    # 3. Submit experiment\n",
    "    config = ExperimentConfig(**config_dict)\n",
    "    job_id = await runner.submit_experiment(test_script, config)\n",
    "    print(f\"✓ Experiment submitted: {job_id}\")\n",
    "    \n",
    "    # 4. Poll for completion\n",
    "    max_polls = 20\n",
    "    poll_count = 0\n",
    "    \n",
    "    while poll_count < max_polls:\n",
    "        status = runner.poll_job(job_id)\n",
    "        \n",
    "        if status.status in ['completed', 'failed']:\n",
    "            break\n",
    "        \n",
    "        await asyncio.sleep(2)\n",
    "        poll_count += 1\n",
    "    \n",
    "    if status.status == 'completed':\n",
    "        print(\"✓ Experiment completed successfully\")\n",
    "        \n",
    "        # 5. Validate results against criteria\n",
    "        results = status.results\n",
    "        if results:\n",
    "            test_model = results.get('test_model', {})\n",
    "            \n",
    "            # Check success criteria\n",
    "            success = True\n",
    "            for metric, threshold in criteria['success_criteria'].items():\n",
    "                if metric == 'min_accuracy':\n",
    "                    if test_model.get('accuracy', 0) < threshold:\n",
    "                        success = False\n",
    "                        print(f\"✗ Accuracy below threshold: {test_model.get('accuracy', 0)} < {threshold}\")\n",
    "                    else:\n",
    "                        print(f\"✓ Accuracy meets threshold: {test_model.get('accuracy', 0)} >= {threshold}\")\n",
    "            \n",
    "            if success:\n",
    "                print(\"✓ All success criteria met\")\n",
    "        \n",
    "        # 6. Get benchmark comparison\n",
    "        comparison = runner.compare_benchmarks(job_id)\n",
    "        if not comparison.empty:\n",
    "            print(\"✓ Benchmark comparison generated\")\n",
    "            print(comparison)\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✗ Experiment failed: {status.error}\")\n",
    "        return False\n",
    "\n",
    "# Run test (uncomment to execute)\n",
    "# success = await test_complete_flow()\n",
    "# print(f\"\\nTest {'PASSED' if success else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete Modal execution runtime with the following capabilities:\n",
    "\n",
    "1. **Modal Serverless Integration**: Spin up serverless instances for ML experiments\n",
    "2. **Experiment Management**: Submit Python scripts with JSON configuration criteria\n",
    "3. **Job Tracking**: Generate job IDs and poll for status updates\n",
    "4. **Benchmark Collection**: Collect and compare metrics across multiple models\n",
    "5. **Test Inputs**: Sample scripts and configurations in `scripts/sample/execution`\n",
    "\n",
    "### Key Components:\n",
    "- `ExperimentRunner`: Main interface for submitting and managing experiments\n",
    "- `JobManager`: Tracks job statuses and results\n",
    "- `ExperimentAPI`: Integration interface for FastAPI server\n",
    "- Modal functions for remote execution\n",
    "- Sample test inputs and validation criteria\n",
    "\n",
    "### Usage:\n",
    "1. Configure Modal authentication\n",
    "2. Load experiment script and configuration\n",
    "3. Submit experiment using `runner.submit_experiment()`\n",
    "4. Poll job status with `runner.poll_job(job_id)`\n",
    "5. Compare benchmarks with `runner.compare_benchmarks(job_id)`\n",
    "6. Fetch artifacts with `runner.get_artifacts(job_id)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
